{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ToukaNour30/VGG-16/blob/main/Another_copy_of_learning_rate_VGG_trial1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow numpy matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHKq9JNNXsWj",
        "outputId": "d0d63a86-203d-4559-ea93-8a07e94156a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle tensorflow numpy matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-AaJPI3vXNPg",
        "outputId": "e966c10b-d18f-4269-b657-7259fffbccf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"<toukanour >\", \"key\":\"<your_kaggle_api_key>\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d rm1000/brain-tumor-mri-scans\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6zZalpAHYAwZ",
        "outputId": "12aad64f-8bc2-4ef3-8e55-cda3597d0d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Dataset URL: https://www.kaggle.com/datasets/rm1000/brain-tumor-mri-scans\n",
            "License(s): CC0-1.0\n",
            "brain-tumor-mri-scans.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_path = \"/content/brain-tumor-mri-scans.zip\"  # Update the path if different\n",
        "extract_path = \"/content/brain-tumor-mri-scans\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3vmxt7P7YNfD",
        "outputId": "902c17dc-ad40-4936-bcbb-9e8ca9abab6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the extracted dataset\n",
        "dataset_path = \"/content/brain-tumor-mri-scans\"\n",
        "\n",
        "# Verify the contents of the dataset\n",
        "print(\"Available files and folders:\")\n",
        "print(os.listdir(dataset_path))\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekf6oYCHYe2R",
        "outputId": "272df821-1f24-446c-a479-82be9488be11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available files and folders:\n",
            "['meningioma', 'healthy', 'glioma', 'pituitary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "j6Qk8-4XZLOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/brain-tumor-mri-scans\"  # Update with your dataset path\n",
        "categories = ['healthy', 'glioma', 'meningioma', 'pituitary']\n",
        "\n",
        "# Parameters\n",
        "img_size = (224, 224)  # Input size for VGG\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "# Function to get dataset\n",
        "def get_dataset(dataset_path, categories):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for category in categories:\n",
        "        folder_path = os.path.join(dataset_path, category)\n",
        "        label = categories.index(category)\n",
        "\n",
        "        for img_name in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, img_name)\n",
        "            image_paths.append(img_path)\n",
        "            labels.append(label)\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "# Load the image paths and labels\n",
        "image_paths, labels = get_dataset(dataset_path, categories)\n",
        "\n",
        "# Split into train and test (70% for training, 30% for testing)\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Print shapes of train and test datasets (number of batches)\n",
        "print(f\"Train dataset batches: {len(train_paths) // batch_size}\")\n",
        "print(f\"Test dataset batches: {len(test_paths) // batch_size}\")\n"
      ],
      "metadata": {
        "id": "rChKtSODhavA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a39da1-d831-412a-eeb3-457c7255dc30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset batches: 76\n",
            "Test dataset batches: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# Add data augmentation layers\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomBrightness(0.2)\n",
        "])\n",
        "\n",
        "# Update the preprocessing function to include augmentation\n",
        "def preprocess_image_with_augmentation(img_path, augment=False):\n",
        "    # Load the image\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)  # Assuming images are in JPEG format\n",
        "    img = tf.image.resize(img, (224, 224))  # Resize to (224, 224)\n",
        "    img = preprocess_input(img)  # Preprocess the image for VGG\n",
        "\n",
        "    # Apply augmentation if specified\n",
        "    if augment:\n",
        "        img = data_augmentation(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "# Apply preprocessing and augmentation to the datasets\n",
        "def create_vgg_dataset(image_paths, labels, batch_size=64, augment=False):\n",
        "    # Create a TensorFlow dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "\n",
        "    # Map function to load, resize, and preprocess the images\n",
        "    dataset = dataset.map(\n",
        "        lambda img_path, label: (preprocess_image_with_augmentation(img_path, augment=augment), label),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Batch and prefetch the dataset\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Convert labels to one-hot encoding for VGG (assuming 4 classes in your dataset)\n",
        "train_labels_one_hot = to_categorical(train_labels, num_classes=4)  # Change `num_classes=4` if necessary\n",
        "test_labels_one_hot = to_categorical(test_labels, num_classes=4)\n",
        "\n",
        "# Create augmented training dataset\n",
        "train_ds_vgg_augmented = create_vgg_dataset(train_paths, train_labels_one_hot, batch_size=64, augment=True)\n",
        "\n",
        "# Use the original preprocessing for validation and test datasets\n",
        "val_ds_vgg = create_vgg_dataset(test_paths, test_labels_one_hot, batch_size=64)\n",
        "test_ds_vgg = create_vgg_dataset(test_paths, test_labels_one_hot, batch_size=64)"
      ],
      "metadata": {
        "id": "0ulo2nD6ie0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n"
      ],
      "metadata": {
        "id": "7w-yEQHQjPbq",
        "outputId": "961eab6f-684d-47a9-c99b-fdb5f2eb1e57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune = 0\n",
        "if fine_tune > 0:\n",
        "  for layer in base_model.layers[:-fine_tune]:\n",
        "    layer.trainable = False\n",
        "else:\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "Mm_1gK2EjcVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes of the batches\n",
        "for images, labels in train_ds_vgg_augmented.take(1):\n",
        "    print(f\"Train batch images shape: {images.shape}\")\n",
        "    print(f\"Train batch labels shape: {labels.shape}\")\n",
        "\n",
        "for images, labels in test_ds_vgg.take(1):\n",
        "    print(f\"Test batch images shape: {images.shape}\")\n",
        "    print(f\"Test batch labels shape: {labels.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "o4zFGH_MjtVS",
        "outputId": "b50eddf0-310f-4e9f-dcda-cb23acb825a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_ds_vgg' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5eb1ba198229>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print shapes of the batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds_vgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train batch images shape: {images.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train batch labels shape: {labels.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds_vgg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(50, activation='relu'),\n",
        "    layers.Dense(20, activation='relu'),\n",
        "    layers.Dense(4, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "3QR9_EKqj1J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **k-Validation**"
      ],
      "metadata": {
        "id": "r53CWCa2g6_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Parameters\n",
        "k = 5  # Number of folds for cross-validation\n",
        "epochs = 10  # Number of epochs\n",
        "batch_size = 64  # Batch size for training\n",
        "\n",
        "# Convert train_paths and train_labels to numpy arrays for compatibility with KFold\n",
        "train_paths_array = np.array(train_paths)\n",
        "train_labels_array = np.array(train_labels_one_hot)\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Lists to store the results for each fold\n",
        "fold_accuracies = []\n",
        "fold_losses = []\n",
        "\n",
        "# Loop over the folds for cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_paths_array)):\n",
        "    print(f\"Training fold {fold + 1}/{k}...\")\n",
        "\n",
        "    # Split the data into training and validation sets for this fold\n",
        "    fold_train_paths = train_paths_array[train_index]\n",
        "    fold_val_paths = train_paths_array[val_index]\n",
        "    fold_train_labels = train_labels_array[train_index]\n",
        "    fold_val_labels = train_labels_array[val_index]\n",
        "\n",
        "    # Create the datasets for this fold\n",
        "    train_ds_fold = create_vgg_dataset(fold_train_paths, fold_train_labels, batch_size)\n",
        "    val_ds_fold = create_vgg_dataset(fold_val_paths, fold_val_labels, batch_size)\n",
        "\n",
        "    # Create a new model for each fold\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(50, activation='relu'),\n",
        "        layers.Dense(20, activation='relu'),\n",
        "        layers.Dense(4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_ds_fold, epochs=epochs, validation_data=val_ds_fold\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the validation data\n",
        "    val_loss, val_acc = model.evaluate(val_ds_fold)\n",
        "    fold_losses.append(val_loss)\n",
        "    fold_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Fold {fold + 1} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# Calculate the average performance across all folds\n",
        "avg_loss = np.mean(fold_losses)\n",
        "avg_acc = np.mean(fold_accuracies)\n",
        "\n",
        "print(f\"Average Validation Loss: {avg_loss:.4f}\")\n",
        "print(f\"Average Validation Accuracy: {avg_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "Q0-80oQ8g5PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **without early stopping**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sERiiGfinQMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_dataset(dataset, validation_split=0.2):\n",
        "    val_size = int(len(dataset) * validation_split)\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset = dataset.take(train_size)\n",
        "    val_dataset = dataset.skip(train_size)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Split the datasets into train and validation datasets\n",
        "train_ds_split, val_ds_split = split_dataset(train_ds_vgg_augmented, validation_split=0.2)\n",
        "\n",
        "new_learning_rate = 0.0001  # Adjust as needed\n",
        "\n",
        "# Compile the model with the new learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=new_learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# EarlyStopping callback\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=3, restore_best_weights=True)\n",
        "\n",
        "#lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds_split,  # Train dataset\n",
        "    epochs=20,\n",
        "    validation_data=val_ds_split,  # Validation dataset\n",
        "    callbacks=[es]\n",
        ")\n"
      ],
      "metadata": {
        "id": "V6FYNNb6j5IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if history and hasattr(history, 'history'):\n",
        "    # Extract the history dictionary\n",
        "    history_dict = history.history\n",
        "\n",
        "    # Ensure the metrics exist in the history dictionary\n",
        "    loss_values = history_dict.get('loss', [])\n",
        "    val_loss_values = history_dict.get('val_loss', [])\n",
        "    accuracy = history_dict.get('accuracy', [])\n",
        "    val_accuracy = history_dict.get('val_accuracy', [])\n",
        "\n",
        "    # Check if metrics are available\n",
        "    if not loss_values or not val_loss_values or not accuracy or not val_accuracy:\n",
        "        print(\"Training history does not contain all required metrics.\")\n",
        "    else:\n",
        "        # Define the range of epochs\n",
        "        epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "        # Create subplots for accuracy and loss\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "        # Plot Training & Validation Accuracy\n",
        "        ax[0].plot(epochs, accuracy, 'r', label='Training accuracy')\n",
        "        ax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "        ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n",
        "        ax[0].set_xlabel('Epochs', fontsize=16)\n",
        "        ax[0].set_ylabel('Accuracy', fontsize=16)\n",
        "        ax[0].legend()\n",
        "\n",
        "        # Plot Training & Validation Loss\n",
        "        ax[1].plot(epochs, loss_values, 'r', label='Training loss')\n",
        "        ax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "        ax[1].set_title('Training & Validation Loss', fontsize=16)\n",
        "        ax[1].set_xlabel('Epochs', fontsize=16)\n",
        "        ax[1].set_ylabel('Loss', fontsize=16)\n",
        "        ax[1].legend()\n",
        "\n",
        "        # Display the plots\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"No training history found or history object is invalid.\")"
      ],
      "metadata": {
        "id": "cS6ZrOcdRYCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pG8wsThZhjz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_predictions = model.predict(test_ds_vgg)\n",
        "test_labels = np.array(test_labels_one_hot)\n",
        "\n",
        "# Convert predictions from one-hot to label (argmax to get class index)\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_predictions)\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
        "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
        "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7Y0V2PdXrCmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "-y9atTssq5bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_acc = model.evaluate(test_ds_vgg)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)\n",
        "\n",
        "# Make predictions on the test set\n",
        "# Iterate over the test dataset and make predictions\n",
        "predictions = model.predict(test_ds_vgg)\n",
        "\n",
        "# Get the true labels (in case you need to print them or evaluate performance)\n",
        "test_labels = np.array(test_labels_one_hot)\n",
        "\n",
        "# Convert predictions from one-hot to class labels\n",
        "predictions_classes = np.argmax(predictions, axis=1)\n",
        "test_labels_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Print predictions and the first test image\n",
        "print(\"Predictions (first sample):\", predictions_classes[0])\n",
        "print(\"True label (first sample):\", test_labels_classes[0])\n",
        "\n",
        "# You need to extract one image from the dataset to display it\n",
        "# Display the first image from the dataset, using the actual image from the dataset (after preprocessing)\n",
        "# Display the first image from the dataset\n",
        "for images, labels in test_ds_vgg.take(1):\n",
        "    # Convert the image to grayscale by averaging the RGB channels\n",
        "    grayscale_image = tf.image.rgb_to_grayscale(images[0])\n",
        "\n",
        "    # Display the grayscale image\n",
        "    plt.imshow(grayscale_image.numpy().squeeze(), cmap=plt.cm.binary)  # Use binary colormap for black and white\n",
        "    plt.title(f\"Predicted: {categories[np.argmax(predictions_classes[0])]}, True: {categories[test_labels_classes[0]]}\")\n",
        "    plt.axis('off')  # Hide axis for better visualization\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "I_rW2GVBhWQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NqMAAfW3dQMi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}